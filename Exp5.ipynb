{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9dbe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 75.0772\n",
      "Epoch 1000, Loss: 2.3654\n",
      "Epoch 2000, Loss: 0.6350\n",
      "Epoch 3000, Loss: 0.3295\n",
      "Epoch 4000, Loss: 0.2147\n",
      "Epoch 5000, Loss: 0.1564\n",
      "Epoch 6000, Loss: 0.1218\n",
      "Epoch 7000, Loss: 0.0990\n",
      "Epoch 8000, Loss: 0.0830\n",
      "Epoch 9000, Loss: 0.0712\n",
      "\n",
      "Word vector for 'language':\n",
      "[ 1.28902794  0.30396502  1.19111738  2.02477027  1.69908188 -0.38941468\n",
      " -0.96064573  0.25709336  2.30716776 -1.84344052]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Step 1: Initialize corpus\n",
    "corpus = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"i love natural language processing\",\n",
    "    \"word2vec is a powerful model for learning word embeddings\"\n",
    "]\n",
    "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
    "\n",
    "# Parameters\n",
    "window_size = 2\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "# Step 2: Build Vocabulary\n",
    "vocab = set(word for sentence in tokenized_corpus for word in sentence)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Step 3: Generate Training Data (CBOW)\n",
    "def generate_training_data(tokenized_corpus, window_size):\n",
    "    training_data = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        for idx, target_word in enumerate(sentence):\n",
    "            context = []\n",
    "            for neighbor in range(-window_size, window_size + 1):\n",
    "                context_pos = idx + neighbor\n",
    "                if neighbor != 0 and 0 <= context_pos < len(sentence):\n",
    "                    context.append(sentence[context_pos])\n",
    "            if context:\n",
    "                training_data.append((context, target_word))\n",
    "    return training_data\n",
    "\n",
    "training_data = generate_training_data(tokenized_corpus, window_size)\n",
    "\n",
    "# Step 4: Initialize weights\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "# Step 5: Softmax function\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# Step 6 & 7: Forward and Backward propagation\n",
    "def forward_backward(context_words, target_word):\n",
    "    global W1, W2\n",
    "    # One-hot encoding for context\n",
    "    x = np.zeros(vocab_size)\n",
    "    for word in context_words:\n",
    "        x[word2idx[word]] += 1\n",
    "    x /= len(context_words)\n",
    "\n",
    "    # Forward pass\n",
    "    h = np.dot(W1.T, x)  # hidden layer\n",
    "    u = np.dot(W2.T, h)  # output layer\n",
    "    y_pred = softmax(u)\n",
    "\n",
    "    # True target\n",
    "    y_true = np.zeros(vocab_size)\n",
    "    y_true[word2idx[target_word]] = 1\n",
    "\n",
    "    # Error\n",
    "    error = y_pred - y_true\n",
    "\n",
    "    # Backward pass\n",
    "    dW2 = np.outer(h, error)\n",
    "    dW1 = np.outer(x, np.dot(W2, error))\n",
    "\n",
    "    # Update weights\n",
    "   \n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "\n",
    "    # Loss (cross-entropy)\n",
    "    loss = -np.log(y_pred[word2idx[target_word]] + 1e-9)\n",
    "    return loss\n",
    "\n",
    "# Step 8: Training\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context_words, target_word in training_data:\n",
    "        loss = forward_backward(context_words, target_word)\n",
    "        total_loss += loss\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Step 9: Get word vectors\n",
    "def get_word_vector(word):\n",
    "    if word in word2idx:\n",
    "        return W1[word2idx[word]]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example\n",
    "print(\"\\nWord vector for 'language':\")\n",
    "print(get_word_vector('language'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fad0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
